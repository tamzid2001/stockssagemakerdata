name: mlops-data

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * *' # run daily at 06:00 UTC

permissions:
  contents: read

jobs:
  fetch:
    runs-on: ubuntu-latest
    env:
      HAS_AWS_CREDS: ${{ secrets.AWS_ACCESS_KEY_ID != '' && secrets.AWS_SECRET_ACCESS_KEY != '' }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Check if today is a weekday
        run: |
          python check_weekday.py today
      
      - name: Calculate date range (last 7 days)
        id: dates
        run: |
          END_DATE=$(date +%Y-%m-%d)
          START_DATE=$(date -d '7 days ago' +%Y-%m-%d)
          echo "start=$START_DATE" >> $GITHUB_OUTPUT
          echo "end=$END_DATE" >> $GITHUB_OUTPUT
          echo "Date range: $START_DATE to $END_DATE"
      
      - name: Fetch market data locally
        run: |
          python fetch_data.py -t AAPL,MSFT,GOOGL -s ${{ steps.dates.outputs.start }} -e ${{ steps.dates.outputs.end }} -i 1d -o output
      
      - name: Upload data artifact (local)
        uses: actions/upload-artifact@v4
        with:
          name: market-data
          path: output
      
      - name: Fetch and upload to S3
        if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule') && env.HAS_AWS_CREDS == 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          python fetch_data_s3.py \
            -t AAPL,MSFT,GOOGL \
            -s ${{ steps.dates.outputs.start }} \
            -e ${{ steps.dates.outputs.end }} \
            -i 1d \
            -b stockscompute \
            -p stock_data/$(date +%Y-%m-%d)/ \
            -r us-east-2
      
      - name: Generate price predictions
        if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule') && env.HAS_AWS_CREDS == 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # Create sample predictions CSV
          cat > predictions.csv <<EOF
          Ticker,Date,Prediction,Confidence
          AAPL,$(date -d '1 day' +%Y-%m-%d),180.50,0.95
          MSFT,$(date -d '1 day' +%Y-%m-%d),425.75,0.94
          GOOGL,$(date -d '1 day' +%Y-%m-%d),140.25,0.92
          EOF
          # Upload predictions to S3
          python predictions.py \
            -f predictions.csv \
            -b stockscompute \
            -p "predictions/$(date +%Y-%m-%d)/" \
            -r us-east-2

      - name: AWS step status
        if: env.HAS_AWS_CREDS != 'true'
        run: |
          echo "AWS credentials are not configured. S3 upload and prediction upload steps were skipped."
          echo "### AWS steps skipped" >> $GITHUB_STEP_SUMMARY
          echo "- Configure repository secrets AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to enable S3 uploads." >> $GITHUB_STEP_SUMMARY

  # Additional job: Data validation
  validate:
    runs-on: ubuntu-latest
    needs: fetch
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: market-data
          path: output
      
      - name: Validate data files
        run: |
          if [ ! -d "output" ]; then
            echo "❌ No output directory found"
            exit 1
          fi
          
          file_count=$(find output -type f -name "*.csv" | wc -l)
          echo "✓ Found $file_count CSV files"
          
          for file in output/*.csv; do
            line_count=$(wc -l < "$file")
            echo "  - $(basename $file): $line_count lines"
          done
      
      - name: Generate summary
        run: |
          echo "## Data Fetch & Predictions Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Time**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Tickers**: AAPL, MSFT, GOOGL" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Collection" >> $GITHUB_STEP_SUMMARY
          echo "- **Format**: CSV with columns (Item_Id, Date, Price)" >> $GITHUB_STEP_SUMMARY
          echo "- **Location**: S3 `stockscompute/stock_data/`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Predictions" >> $GITHUB_STEP_SUMMARY
          echo "- **Format**: CSV with custom columns (user-created)" >> $GITHUB_STEP_SUMMARY
          echo "- **Location**: S3 \`stockscompute/predictions/\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Method**: Local CSV file upload" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status**: ✓ Complete" >> $GITHUB_STEP_SUMMARY
